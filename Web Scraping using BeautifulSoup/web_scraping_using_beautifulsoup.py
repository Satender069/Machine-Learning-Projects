# -*- coding: utf-8 -*-
"""Web Scraping using BeautifulSoup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1770h4at5pWNdM9XRiGq7j_fLPjHauLFA

# **Web Scraping using BeautifulSoup**

## <font color=purple>*A). Scraping from newspaper "The Asian Age"*</font>

This newspaper is scraped in depth going from page to page in various categories from home page. 

I have tried automate the process as much as possible.
"""

import requests
from bs4 import BeautifulSoup as bs

news_page2 = requests.get("https://www.asianage.com/india")
content2   = news_page2.content
soup2 = bs(content2,'html.parser')
list_soup_children2 = list(soup2.children)

tags = list_soup_children2[2]
pages = tags.select("li li")[:17]

# Getting links of news pages regarding different categories of news

link_category = []
for page in pages:
    link_category.append("https://www.asianage.com"+page.find('a').get('href'))
    
link_category

page_no = [number for number in range(1,10)]  # Number of pages to scan from each category. 
                                              #Remember that some category have very less pages to passing large value might show error.
dict_final = {"Title":[], "Content":[]}

for page_cat in link_category:
    for num in page_no:
        new_page = requests.get(page_cat+'?pg={}'.format(num))
        soup_np = bs(new_page.content,'html.parser')
        tags_np = list(soup_np.children)[2]
        articles = tags_np.select('h2.costly a') # articles list with 'a' tag containing their urls
        
        # WORKING WITH SPECIFIC STORY PAGE
    
        for story in articles:
            link_story = 'https://www.asianage.com'+story.get('href')
            story_page = requests.get(link_story)
            soup_story = bs(story_page.content,'html.parser')
            tag_story = list(soup_story.children)[2]
            
            # GETTING TITLE OF THE STORY
            title = tag_story.find('title').get_text()
            dict_final["Title"].append(title)
            
            # GETTING CONTENT OF THE STORY
            paras_list = tag_story.select('div.storyBody p')
            content=''
            for paras in paras_list:
                content += paras.get_text()
            dict_final["Content"].append(content)

import pandas as pd
table = pd.DataFrame(dict_final)      # Creating Data Frame of resulting values
table

"""## <font color=purple>*B). Scraping from newspaper "International Business Times"*</font>

Code for above newspaper scrapes large amount of data one time, but we can run below program daily and it will add latest stories from newspaper in existing dictionary. It extracts main stories from front page only rather than scraping in depth for old news as above program does.

### Note:  Do not run below cell if you want to add new data to older one.
"""

dict_main = {"Title":[],"Content":[]}

news_page = requests.get("https://www.ibtimes.co.in/")
content   = news_page.content
soup = bs(content,'html.parser')
list_soup_children = list(soup.children)
[type(item) for item in list_soup_children]  # Checking for tags only in soup's children

import json
tags = list_soup_children[2]
article_list = tags.select('article h3 a') # getting to 'a' tag inside heading level 3 i.e. 'h3' which itself is inside 'article' tag

url_list = []
for i in range(len(article_list)):
    url_list.append(article_list[i].get('href'))          # Collecting url's of news from 'href' inside 'a' tags 
    dict_main["Title"].append(article_list[i].get_text())         # Collecting news headings 
    

for link in url_list:
    get_page = requests.get(link)
    soup = bs(get_page.content,'html.parser')                       # creating soup for each news webpage collected from main page.
    script = soup.find('script', {'type' : 'application/ld+json'})  # Content is inside 'script' tag of type 'application/ld+json'.
    parsing = json.loads(script.string)                             # Providing string value inside 'loads' module of 'json' library which  
                                                                    # returns dictionary with different elements from 'script' tag on the website.
    dict_main["Content"].append(parsing['articleBody'])             # Content is the value of key 'articleBody' in dictionary named 'parsing'

table2 = pd.DataFrame(dict_main)      # Creating Data Frame of resulting values
table2

"""## Combining data from both the newspaper"""

# ADDING BOTH DICTIONARIES 
dict_final["Title"].extend(dict_main["Title"])
dict_final["Content"].extend(dict_main["Content"])

new_dict = {"Title":dict_final["Title"],"Content":dict_final["Content"]} #

table_final = pd.DataFrame(new_dict)
table_final

from google.colab import files

table_final.to_csv("scraped_data.csv")
files.download('scraped_data.csv')